import streamlit as st
import feedparser
from newspaper import Article
import openai
from dotenv import load_dotenv
import os
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

load_dotenv()
# ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI()


def summarize_by_url_only(url):
    prompt = f"""
ì´ ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì°¸ê³ í•´ ë¯¸êµ­ ê²½ì œ ìš”ì•½ì„ í•´ì¤˜. íŠ¸ëŸ¼í”„ ì •ì±…ê³¼ ì£¼ì‹ì‹œì¥ ê´€ë ¨ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¤‘ì  ë¶„ì„í•´ì¤˜:

URL: {url}
"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=800
    )
    return response.choices[0].message.content

def fallback_extract_article(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì •: <p> íƒœê·¸ ë‹¤ ëª¨ì•„ì„œ ì—°ê²°
        paragraphs = soup.find_all('p')
        text = ' '.join(p.get_text() for p in paragraphs)
        if len(text.strip()) < 300:
            return ""
        return text
    except:
        return ""
    
def get_article_text(url):
    text = extract_article(url)
    if not text:
        text = fallback_extract_article(url)
    return text

def get_article_summary(url):
    text = extract_article(url)
    if text:
        return summarize_article(text)
    else:
        return summarize_by_url_only(url)
    
# --- í•¨ìˆ˜: ë‰´ìŠ¤ RSS ê°€ì ¸ì˜¤ê¸° ---
def get_news_entries():
    url = "https://news.google.com/rss/search?q=US+economy+when:1d&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(url)
    return feed.entries[:5]

# --- í•¨ìˆ˜: ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ---
def extract_article(url):
    article = Article(url)
    try:
        article.download()
        article.parse()
        # print(article.html)
        return article.text
    except:
        return ""

# --- í•¨ìˆ˜: OpenAIë¡œ ìš”ì•½ ë° ë¶„ì„ ---
def summarize_article(text):
    prompt = f"""
ë‹¤ìŒì€ ë¯¸êµ­ ê²½ì œ ê¸°ì‚¬ì…ë‹ˆë‹¤. ë‹¤ìŒ í•­ëª©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:

1. ê¸°ì‚¬ í•µì‹¬ ìš”ì•½ (3~4ì¤„)
2. íŠ¸ëŸ¼í”„ì˜ ê²½ì œ ì •ì±… ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ìƒì„¸ ë¶„ì„
3. ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ë¶„ì„ (S&P 500, ë‚˜ìŠ¤ë‹¥, ë‹¤ìš° í¬í•¨)
4. ê²½ì œ íë¦„ì— ëŒ€í•œ ì‹œì‚¬ì  ì •ë¦¬

ê¸°ì‚¬ ì›ë¬¸:
{text}
"""
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=1000
    )
    return response.choices[0].message.content

# --- Streamlit ì•± ì‹œì‘ ---
st.set_page_config(page_title="ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½", layout="wide")
st.title("ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½ ë° ë¶„ì„")
st.markdown("ìµœê·¼ ë¯¸êµ­ ê²½ì œ ë‰´ìŠ¤ ì¤‘ **íŠ¸ëŸ¼í”„ ê²½ì œ ì •ì±…**ê³¼ **ì£¼ì‹ ì‹œì¥ ìƒí™©**ì— ì´ˆì ì„ ë§ì¶° ìš”ì•½í•©ë‹ˆë‹¤.")

# ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
with st.spinner("ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
    news_entries = get_news_entries()

# ê¸°ì‚¬ ì„ íƒ
for idx, entry in enumerate(news_entries):
    with st.expander(f"{entry.title}"):
        st.write(f"[ê¸°ì‚¬ ë§í¬]({entry.link})")
        with st.spinner("ìš”ì•½ ì¤‘..."):
            # content = extract_article(entry.link)
            content = get_article_summary(entry.link)
            if content:
                summary = summarize_article(content)
                st.markdown(summary)
            else:
                st.warning("âŒ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

